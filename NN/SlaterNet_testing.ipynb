{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5224e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import erfc, exp, factorial, pi, sqrt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef782b3",
   "metadata": {},
   "source": [
    "### Static Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5c8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Simulation constants & helper functions\n",
    "a_m = 8.031  # nm   moiré lattice constant, supercell length, from paper (the lattice mismatch between WSe2 and WS2)\n",
    "V0 = 15.0  # meV  from paper\n",
    "eps_r = 5.0  # dielectric constant in paper\n",
    "e2_4pieps0 = 14.399645  # meV·nm (|e|²/4πϵ0)\n",
    "hbar2_over_2m = 108.857  # meV·nm² in paper\n",
    "phi = np.pi / 4  # phase of the moiré potential from paper\n",
    "n_sup = 3  # 3×3 super-cell\n",
    "N_e = 6  # # electrons (= # occupied orbitals)\n",
    "cut = 2  # |G|≤cut·|g| # cutoff for the plane-wave basis (G vectors) in the Hamiltonian\n",
    "\n",
    "# --- Ewald parameters (renamed) ---\n",
    "ew_alpha = 0.35  # nm⁻²  (splitting)\n",
    "r_cut = 2.5  # real-space cutoff in L units\n",
    "k_cut = 5  # k-space cutoff in 2π/L units\n",
    "L = n_sup * a_m  # nm   PBC box length used in Ewald\n",
    "\n",
    "\n",
    "def a_vectors(a_m):\n",
    "    \"\"\"Generates the 3 shortest moiré reciprocal vectors G_1,2,3, 60° apart, six-fold symmetry\"\"\"\n",
    "    a1 = a_m * np.array([1.0, 0.0])\n",
    "    a2 = a_m * np.array([0.5, np.sqrt(3.0) / 2.0])\n",
    "    a3 = -(a1 + a2)  # optional third vector (120° w.r.t. a1)\n",
    "    return np.array([a1, a2, a3])  # same interface style as your b_vectors()\n",
    "\n",
    "\n",
    "def b_vectors(a_m):\n",
    "    \"\"\"from paper: g_j = (4*pi / sqrt(3) / a_m) * [cos(2*pi*j/3), sin(2*pi*j/3)], for j=1,2,3\"\"\"\n",
    "    g_list = []\n",
    "    prefac = 4 * np.pi / (np.sqrt(3) * a_m)\n",
    "    for j in range(1, 4):  # j = 1, 2, 3\n",
    "        angle = 2 * np.pi * j / 3\n",
    "        g = prefac * np.array([np.cos(angle), np.sin(angle)])\n",
    "        g_list.append(g)\n",
    "    # print(\"g list\", g_list)\n",
    "    return np.array(g_list)  # returns [g1, g2, g3]\n",
    "\n",
    "\n",
    "# real-space vectors of an n×n super-cell   (n = 3 here since we have 3x3 supercell)\n",
    "\n",
    "\n",
    "def supercell_vectors(n, a_m):\n",
    "    a1, a2 = a_vectors(a_m)\n",
    "    return n * a1, n * a2\n",
    "\n",
    "\n",
    "# ξ_M : configuration-independent Madelung constant\n",
    "def madelung_offset(\n",
    "    alpha=ew_alpha,  # α = 1/(4η²)\n",
    "    r_lim=r_cut,\n",
    "    k_lim=k_cut,\n",
    "    L=L,\n",
    "):\n",
    "    \"\"\"Compute ξ_M in Eq. (A12) Returns a scalar (dimensionless).  Multiply by e²/4πϵ₀ϵ_r later.\"\"\"\n",
    "    eta = 0.5 / np.sqrt(alpha)  # because α = 1/(4η²)\n",
    "    area = L * L\n",
    "    k0 = 2.0 * np.pi / L\n",
    "    # ---- Real-space images   Σ_{L≠0} erfc(|L|/2η)/|L|\n",
    "    rsum = 0.0\n",
    "    maxn = int(np.ceil(r_lim))\n",
    "    for nx in range(-maxn, maxn + 1):\n",
    "        for ny in range(-maxn, maxn + 1):\n",
    "            if nx == 0 and ny == 0:\n",
    "                continue\n",
    "            Rvec = np.array([nx, ny]) * L\n",
    "            R = np.linalg.norm(Rvec)\n",
    "            if R > r_lim * L:\n",
    "                continue\n",
    "            rsum += erfc(R / (2.0 * eta)) / R\n",
    "    # ---- Reciprocal-space images   (2π/Area) Σ_{G≠0} e^{−η²G²}/G\n",
    "    ksum = 0.0\n",
    "    for mx in range(-k_lim, k_lim + 1):\n",
    "        for my in range(-k_lim, k_lim + 1):\n",
    "            if mx == 0 and my == 0:\n",
    "                continue\n",
    "            Gvec = np.array([mx, my]) * k0\n",
    "            G = np.linalg.norm(Gvec)\n",
    "            ksum += np.exp(-((eta * G) ** 2)) / G\n",
    "    ksum *= 2.0 * np.pi / area\n",
    "    xi0_L = 1.0 / (eta * np.sqrt(np.pi))  # ---- ξ^L_0 term   1 / (η √π)\n",
    "    return rsum + ksum - xi0_L  # ---- ξ_M (Eq. A12)\n",
    "\n",
    "\n",
    "ξ_M = madelung_offset() * e2_4pieps0 / eps_r  # compute once and store — units:   meV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bef8ee",
   "metadata": {},
   "source": [
    "### Defining the SlaterNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa647c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # W^(l+1) h^l + b^(l+1)\n",
    "        self.Wl_1p = nn.Linear(embed_dim, embed_dim)\n",
    "        # (nonlinear) hyperbolic tangent activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, hl: torch.Tensor) -> torch.Tensor:\n",
    "        # input should be of shape (N, embed_dim): h^l + tanh( W^(l+1) h^l + b^(l+1) )\n",
    "        return hl + self.tanh(self.Wl_1p(hl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53be6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlaterNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, G_vectors: np.ndarray, N: int, embed_dim: int = 4, num_layers: int = 3\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # N is the number of electrons.\n",
    "        self.N = N\n",
    "\n",
    "        # get G vectors\n",
    "        G_vectors = torch.from_numpy(G_vectors).float()\n",
    "        self.G1_T = G_vectors[0].unsqueeze(-1)\n",
    "        self.G2_T = G_vectors[1].unsqueeze(-1)\n",
    "\n",
    "        # input embedding matrix: projects 4 features to embed_dim\n",
    "        self.W_0 = nn.Linear(4, embed_dim, bias=False)\n",
    "        self.MLP_layers = nn.ModuleList(  # MLP layers\n",
    "            [FeedForwardLayer(embed_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # matrix to hold the projection vectors (complex projectors for orbital)\n",
    "        # w_2j and w_2j+1 for j = 0, ... N-1\n",
    "        self.complex_proj = nn.Parameter(\n",
    "            torch.complex(\n",
    "                real=torch.randn(embed_dim, N), imag=torch.randn(embed_dim, N)\n",
    "            )\n",
    "        )\n",
    "        self.denominator = sqrt(factorial(N))\n",
    "\n",
    "    def forward(self, R: torch.Tensor) -> torch.Tensor:  # R should be of shape (N, 2)\n",
    "        # compute the periodic features\n",
    "        G1_R = torch.matmul(R, self.G1_T)\n",
    "        G2_R = torch.matmul(R, self.G2_T)\n",
    "        features_R = torch.cat(\n",
    "            (torch.sin(G1_R), torch.sin(G2_R), torch.cos(G1_R), torch.cos(G2_R)), dim=1\n",
    "        )  # shape should now be (N, 4)\n",
    "\n",
    "        # embed in higher_dimensional space to get h^0\n",
    "        h = self.W_0(features_R)\n",
    "\n",
    "        # pass through MLP layers\n",
    "        for layer in self.MLP_layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        # form complex matrix as in Eq. 2\n",
    "        WF_matrix = torch.matmul(h.to(torch.complex64), self.complex_proj)\n",
    "\n",
    "        # compute determinant\n",
    "        sign, log_abs_det = torch.linalg.slogdet(WF_matrix)\n",
    "        determinant = sign * torch.exp(log_abs_det)\n",
    "        result = determinant / self.denominator\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059e38f",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df118eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SlaterNet(\n",
       "  (W_0): Linear(in_features=4, out_features=5, bias=False)\n",
       "  (MLP_layers): ModuleList(\n",
       "    (0-2): 3 x FeedForwardLayer(\n",
       "      (Wl_1p): Linear(in_features=5, out_features=5, bias=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = SlaterNet(G_vectors=b_vectors(a_m), N=10, embed_dim=5, num_layers=3)\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3ac754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor(-7.2255e-36+9.2948e-36j, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "R = torch.randn(10, 2)\n",
    "print(R.dtype)\n",
    "phi_HF = test_model(R)\n",
    "print(phi_HF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7d2c9",
   "metadata": {},
   "source": [
    "### Helper functions to compute local energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b39655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Ewald helpers (using ew_alpha) ----------\n",
    "def pairwise_real_space(R, alpha=ew_alpha, r_lim=r_cut, L=L):\n",
    "    \"\"\"Short-range (real-space) Ewald sum (equation A6 from the paper):\n",
    "    E_real = ½ ∑_{i≠j} ∑_L erfc(√α·r_{ij}^L) / r_{ij}^L.,\n",
    "    where α = 1/(4η²), and r_{ij}^L = |r_i - r_j + L|\"\"\"\n",
    "    N, E = len(R), 0.0  # N:number particles\n",
    "    maxn = int(np.ceil(r_lim))  # summing over neighbor cells from -max_n to +max_n\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):  # loop over ½ ∑_{i≠j}\n",
    "            for nx in range(-maxn, maxn + 1):  # loop over n_x n_y\n",
    "                for ny in range(-maxn, maxn + 1):\n",
    "                    dr = R[i] - R[j] + np.array([nx, ny]) * L  # dr = r_i - r_j + n·L\n",
    "                    r = np.linalg.norm(dr)  # r = |dr|\n",
    "                    if r < 1e-9 or r > r_lim * L:\n",
    "                        continue  # Skip self‐interaction (r≈0) or beyond cutoff r_lim·L\n",
    "                    E += erfc(sqrt(alpha) * r) / r  # α = 1/(4η²) we choose\n",
    "    return E\n",
    "\n",
    "\n",
    "def structure_factor(R, k):  # Σ e^{ik·r}\n",
    "    \"\"\"Structure factor S(k) = Σ e^{ik·r} (sum over all particles)\"\"\"\n",
    "    phase = R @ k\n",
    "    return np.sum(np.cos(phase)) + 1j * np.sum(np.sin(phase))\n",
    "\n",
    "\n",
    "def reciprocal_space(R, alpha=ew_alpha, k_lim=k_cut, L=L):\n",
    "    \"\"\"Long-range (reciprocal‐space) Ewald sum (equation A7 and A10 from the paper):\n",
    "    E_recip = (π/V) ∑_{k≠0} [ e^{-k²/(4α)} / k² ] |S(k)|²\n",
    "    with V = L² in 2D, fast convergence.\"\"\"\n",
    "    area, k0 = L * L, 2 * pi / L\n",
    "    E = 0.0\n",
    "    # 2) Sum over discrete wavevectors q = (m_x, m_y)·(2π/L), the paper has ∑_{q≠0};\n",
    "    # here we loop m_x, m_y ∈ [−k_lim,…,+k_lim]\n",
    "    for mx in range(-k_lim, k_lim + 1):\n",
    "        for my in range(-k_lim, k_lim + 1):\n",
    "            if mx == 0 and my == 0:\n",
    "                continue  # skip the q = 0 term\n",
    "            k = np.array([mx, my]) * k0  # q_vec = (m_x, m_y)·(2π/L)\n",
    "            k2 = k @ k  # q² = |q_vec|² = q_x² + q_y²\n",
    "            E += (\n",
    "                exp(-k2 / (4 * alpha)) * abs(structure_factor(R, k)) ** 2 / k2\n",
    "            )  # factor e^{–q²/(4α)} # e^{-q²/(4α)}/q² · |S(q)|²\n",
    "    return (pi / area) * E\n",
    "\n",
    "\n",
    "def self_energy(N, alpha=ew_alpha):\n",
    "    \"\"\"(equation A12 from the paper, Madelung constant) Self‐interaction correction: E_self = - ∑_i (√α / √π) · q_i²\n",
    "    Here q_i are unit charges, so E_self = -N·(√α/√π).\"\"\"\n",
    "    return -sqrt(alpha / pi) * N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b17bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moire_potential(r, a_m=a_m, V0=V0, phi=phi):\n",
    "    \"\"\"V(r) = -2*V0*sum_{j=1}^{3} cos(g_j · r + phi)where g_j are 3 reciprocal lattice vectors (from paper).\"\"\"\n",
    "    G = torch.from_numpy(\n",
    "        np.array(b_vectors(a_m), dtype=np.float32)\n",
    "    )  # Get the three reciprocal vectors, shape (3, 2)\n",
    "    phase = torch.matmul(r, G.T) + phi  # r @ G.T + phi\n",
    "    one_electron_moire = -2 * V0 * torch.sum(torch.cos(phase), dim=-1)\n",
    "    return torch.sum(one_electron_moire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e810d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coulomb_ewald_2D(R):\n",
    "    \"\"\"\n",
    "    Full 2-D Ewald energy for the set of positions R of shape (N, 2).\n",
    "    Now includes ½ Σ_b ξ_M  so the result matches Eq. (A11) exactly.\n",
    "    E_total = (e²/(4πϵ₀ ε_r))·( E_real + E_recip + E_self )\n",
    "    \"\"\"\n",
    "    N = len(R)\n",
    "    # position-dependent part (your original implementation)\n",
    "    E_config = (\n",
    "        (pairwise_real_space(R) + reciprocal_space(R) + self_energy(N))\n",
    "        * e2_4pieps0\n",
    "        / eps_r\n",
    "    )\n",
    "    # constant Madelung shift\n",
    "    return E_config + 0.5 * N * ξ_M\n",
    "\n",
    "\n",
    "def energy_static(R):\n",
    "    \"\"\"V_ext + V_ee  (independent of Ψ).\"\"\"\n",
    "    return moire_potential(R)  # + coulomb_ewald_2D(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6284d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian_hessian(net, R):\n",
    "    # first, determine the shape of R\n",
    "    N, D = R.shape  # N: number of electrons, D: dimensions (2D here)\n",
    "    R_flattened = R.reshape(N * D)  # Flatten R to (N * D)\n",
    "\n",
    "    # Define helper runctions that take an input of shape (N * D) and output a real scalar:\n",
    "    def fn_real(X):\n",
    "        return net(X.reshape(N, D)).real\n",
    "\n",
    "    def fn_imag(X):\n",
    "        return net(X.reshape(N, D)).imag\n",
    "\n",
    "    # Compute the Hessian matrix w.r.t. both functions\n",
    "    real_Hessian = torch.func.hessian(fn_real)(R_flattened)\n",
    "    imag_Hessian = torch.func.hessian(fn_imag)(R_flattened)\n",
    "\n",
    "    # Compute the Laplacian as the trace of the Hessian\n",
    "    real_laplacian = torch.trace(real_Hessian)\n",
    "    imag_laplacian = torch.trace(imag_Hessian)\n",
    "    return real_laplacian + (1j * imag_laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba8b7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the local energy\n",
    "def local_energy(net, R):\n",
    "    \"\"\"Local energy using the Hessian-based Laplacian.\"\"\"\n",
    "\n",
    "    psi = net(R)  # complex scalar\n",
    "\n",
    "    # The only change: use compute_laplacian_hessian instead of nested loops.\n",
    "    lap_psi = compute_laplacian_hessian(net, R)  # complex\n",
    "\n",
    "    # Compute kinetic energy term: T = -ħ²/2m ∇²ψ / ψ\n",
    "    kin = -(hbar2_over_2m * (lap_psi / psi)).real\n",
    "\n",
    "    # Compute potential energy term: V = V_ext + V_ee\n",
    "    V = energy_static(R)\n",
    "    total_energy = kin + V\n",
    "    return total_energy, psi  # return both energy and wavefunction value\n",
    "\n",
    "\n",
    "vmapped_local_energy = torch.func.vmap(local_energy, in_dims=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ecd32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 10, 2)\n",
    "e, psi = vmapped_local_energy(test_model, x)\n",
    "print(e.shape, psi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0433753",
   "metadata": {},
   "source": [
    "### VMC Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e6fadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3631718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_sampler(net, R_init, n_steps=200, step_size=0.1):\n",
    "    \"\"\"Metropolis-Hastings sampler to draw samples ~ |Ψ|².\n",
    "    Inputs:\n",
    "      - net: SlaterNet model\n",
    "      - R_init: torch.Tensor (N_e,2), starting positions\n",
    "      - n_steps: number of MCMC steps\n",
    "      - step_size: Gaussian proposal standard deviation\n",
    "    Returns:\n",
    "      - samples: list of torch.Tensor configurations (one per step)\"\"\"\n",
    "\n",
    "    R = R_init.clone().to(device)\n",
    "    psi_sq = torch.abs(net(R)) ** 2  # Compute initial |Ψ|²\n",
    "    samples = []\n",
    "    for _ in range(n_steps):\n",
    "        for i in range(R.shape[0]):  # propose move for each electron\n",
    "            R_prop = R.clone()\n",
    "            R_prop[i] += step_size * torch.randn_like(\n",
    "                R[i]\n",
    "            )  # Gaussian random displacement\n",
    "            R_prop[i] = R_prop[i] % L  # Enforce PBC in a box of length L\n",
    "            psi_sq_prop = (\n",
    "                torch.abs(net(R_prop)) ** 2\n",
    "            )  # evalualte |Ψ|² at the proposed R\n",
    "            if (psi_sq_prop / (psi_sq + 1e-12)) > torch.rand(1, device=device):\n",
    "                R[i] = R_prop[i]\n",
    "                psi_sq = psi_sq_prop\n",
    "        samples.append(R.clone())\n",
    "    return samples\n",
    "\n",
    "\n",
    "# training loop with Variational Monte Carlo\n",
    "def train_vmc(net, n_iter=200, n_walkers=8, n_steps=200, step_size=0.1):\n",
    "    \"\"\"VMC training with a decaying learning rate: η(t) = η_0 * (1 + t/t0)^(-1) (page 13)\"\"\"\n",
    "\n",
    "    η_0 = 1e-4  # initial learning rate (Table II)\n",
    "    t0 = 1e5  # decay “time constant”\n",
    "    rho = 5.0  # clipping threshold for local energy\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=η_0)\n",
    "    # Adam will perform parameter updates θ ← θ – η·∇_θ L\n",
    "\n",
    "    # Each walker is one configuration R of N_e electrons in 2D, drawn uniformly in [0, L)^2\n",
    "    walkers = [L * torch.rand(net.N, 2, device=device) for _ in range(n_walkers)]\n",
    "\n",
    "    # Main VMC loop ──────────────────────────────────────────────────────────────────\n",
    "    for it in range(1, n_iter + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # — Update the learning rate —\n",
    "        lr_t = η_0 * (1 + it / t0) ** -1\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr_t\n",
    "\n",
    "        # Prepare lists to collect data over all walkers & steps\n",
    "        new_walkers = []  # to hold final config of each walker\n",
    "        samples = []  # to hold all samples for each walker\n",
    "\n",
    "        # Sample from each walker\n",
    "        for w in walkers:\n",
    "            samples.extend(mcmc_sampler(net, w, n_steps, step_size))\n",
    "            new_walkers.append(samples[-1])\n",
    "\n",
    "        # Convert list of samples to a tensor of shape (n_walkers * n_steps, N_e, 2)\n",
    "        samples = torch.stack(samples)\n",
    "\n",
    "        # compute the local energy for all the samples\n",
    "        E_full, psi = vmapped_local_energy(net, samples)\n",
    "\n",
    "        # reshape and process the energies\n",
    "        E_full = E_full.squeeze(-1)\n",
    "        E_clip = torch.clamp(E_full, -rho, +rho)\n",
    "        logpsi = torch.log(torch.abs(psi.squeeze(-1)) + 1e-12)\n",
    "\n",
    "        # true mean energy (for reporting)\n",
    "        E_mean_full = E_full.mean()\n",
    "\n",
    "        # clipped mean energy (for loss)\n",
    "        E_mean_clip = E_clip.mean().detach()\n",
    "\n",
    "        # VMC loss uses clipped energies\n",
    "        loss = torch.mean((E_clip - E_mean_clip) * logpsi)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        walkers = new_walkers\n",
    "        print(\n",
    "            f\"Iter {it:3d}/{n_iter:3d} | <E> = {E_mean_full.item():.6f} meV | lr = {lr_t:.3e}\"\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken: {end_time - start_time}\\n\")\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa084d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   1/  5 | <E> = 4471858208440320.000000 meV | lr = 1.000e-04\n",
      "Time taken: 14.542210102081299\n",
      "Iter   2/  5 | <E> = -14814428528640.000000 meV | lr = 1.000e-04\n",
      "Time taken: 17.229722023010254\n",
      "Iter   3/  5 | <E> = -596572031156224.000000 meV | lr = 1.000e-04\n",
      "Time taken: 16.84843897819519\n",
      "Iter   4/  5 | <E> = 8836599932518400.000000 meV | lr = 1.000e-04\n",
      "Time taken: 14.976014137268066\n",
      "Iter   5/  5 | <E> = -11950712946688.000000 meV | lr = 1.000e-04\n",
      "Time taken: 16.461782932281494\n"
     ]
    }
   ],
   "source": [
    "# train to show time\n",
    "net = SlaterNet(\n",
    "    G_vectors=b_vectors(a_m), N=N_e, embed_dim=64, num_layers=3\n",
    ")  # -----number layers: 3; Perceptron dim: 64\n",
    "trained_net = train_vmc(\n",
    "    net,\n",
    "    n_iter=5,  # paper says 150000 ------- Training iterations\n",
    "    n_walkers=16,\n",
    "    n_steps=256,\n",
    "    step_size=0.05,\n",
    ")\n",
    "\n",
    "# ------n_walkers * n_steps = 4096 (MCMC batch size) namely gather 4096 (𝑅,𝐸_loc, lnΨ) where\n",
    "# “n_walkers”：how many independent MCMC chains we run in parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchbearer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
