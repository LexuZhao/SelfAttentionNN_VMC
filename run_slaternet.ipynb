{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffa1728",
   "metadata": {},
   "source": [
    "## run slaternet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01bd974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from math import erfc, sqrt, pi, exp\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01c31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Simulation constants & helper functions\n",
    "a_m   = 8.031          # nm   moiré lattice constant, supercell length, from paper (the lattice mismatch between WSe2 and WS2)\n",
    "V0    = 15.0         # meV  from paper\n",
    "eps_r = 5.0         # dielectric constant in paper\n",
    "e2_4pieps0 = 14.399645  # meV·nm (|e|²/4πϵ0)\n",
    "hbar2_over_2m = 108.857 # meV·nm² in paper\n",
    "phi = np.pi/4   # phase of the moiré potential from paper\n",
    "n_sup = 3            # 3×3 super-cell\n",
    "N_e   = 6               # # electrons (= # occupied orbitals)\n",
    "cut   = 2  # |G|≤cut·|g| # cutoff for the plane-wave basis (G vectors) in the Hamiltonian\n",
    "\n",
    "# --- Ewald parameters (renamed) ---\n",
    "ew_alpha = 0.35      # nm⁻²  (splitting)\n",
    "r_cut    = 2.5       # real-space cutoff in L units\n",
    "k_cut    = 5         # k-space cutoff in 2π/L units\n",
    "L     = n_sup * a_m  # nm   PBC box length used in Ewald\n",
    "\n",
    "def a_vectors(a_m):\n",
    "    \"\"\"Generates the 3 shortest moiré reciprocal vectors G_1,2,3, 60° apart, six-fold symmetry\"\"\"\n",
    "    a1 = a_m * np.array([1.0,                0.0           ])\n",
    "    a2 = a_m * np.array([0.5,  np.sqrt(3.0) / 2.0          ])\n",
    "    a3 = -(a1 + a2)        # optional third vector (120° w.r.t. a1)\n",
    "    return [a1, a2, a3]    # same interface style as your b_vectors()\n",
    "\n",
    "\n",
    "def b_vectors(a_m):\n",
    "    \"\"\"from paper: g_j = (4*pi / sqrt(3) / a_m) * [cos(2*pi*j/3), sin(2*pi*j/3)], for j=1,2,3\"\"\"\n",
    "    g_list = []\n",
    "    prefac = 4 * np.pi / (np.sqrt(3) * a_m)\n",
    "    for j in range(1, 4):  # j = 1, 2, 3\n",
    "        angle = 2 * np.pi * j / 3\n",
    "        g = prefac * np.array([np.cos(angle), np.sin(angle)])\n",
    "        g_list.append(g)\n",
    "    # print(\"g list\", g_list)\n",
    "    return g_list  # returns [g1, g2, g3]\n",
    "\n",
    "# real-space vectors of an n×n super-cell   (n = 3 here since we have 3x3 supercell)\n",
    "def supercell_vectors(n, a_m):\n",
    "    a1, a2 = a_vectors(a_m)\n",
    "    return n*a1, n*a2\n",
    "\n",
    "\n",
    "def moire_potential(r, a_m = a_m, V0 = V0, phi = phi):\n",
    "    \"\"\" V(r) = -2*V0*sum_{j=1}^{3} cos(g_j · r + phi)where g_j are 3 reciprocal lattice vectors (from paper).\"\"\"\n",
    "    G = np.array(b_vectors(a_m))  # Get the three reciprocal vectors, shape (3,2)\n",
    "    phase = np.dot(r, G.T) + phi  # r @ G.T + phi\n",
    "    one_electron_moire = -2 * V0 * np.sum(np.cos(phase), axis=-1)\n",
    "    return np.sum(one_electron_moire)\n",
    "\n",
    "# ---------- Ewald helpers (using ew_alpha) ----------\n",
    "def pairwise_real_space(R, alpha=ew_alpha, r_lim=r_cut, L=L):\n",
    "    \"\"\" Short‐range (real‐space) Ewald sum (equation A6 from the paper):\n",
    "    E_real = ½ ∑_{i≠j} ∑_L erfc(√α·r_{ij}^L) / r_{ij}^L.,\n",
    "    where α = 1/(4η²), and r_{ij}^L = |r_i - r_j + L|\"\"\"\n",
    "    N, E = len(R), 0.0 #N:number particles\n",
    "    maxn = int(np.ceil(r_lim)) # summing over neighbor cells from -max_n to +max_n\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N): # loop over ½ ∑_{i≠j}\n",
    "            for nx in range(-maxn,maxn+1): #loop over n_x n_y\n",
    "                for ny in range(-maxn,maxn+1):\n",
    "                    dr = R[i]-R[j]+np.array([nx,ny])*L # dr = r_i - r_j + n·L\n",
    "                    r  = np.linalg.norm(dr) # r = |dr|\n",
    "                    if r<1e-9 or r>r_lim*L: continue # Skip self‐interaction (r≈0) or beyond cutoff r_lim·L\n",
    "                    E += erfc(sqrt(alpha)*r)/r # α = 1/(4η²) we choose\n",
    "    return E\n",
    "\n",
    "def structure_factor(R,k):    # Σ e^{ik·r}\n",
    "    \"\"\"Structure factor S(k) = Σ e^{ik·r} (sum over all particles)\"\"\"\n",
    "    phase = R @ k\n",
    "    return np.sum(np.cos(phase))+1j*np.sum(np.sin(phase))\n",
    "\n",
    "def reciprocal_space(R, alpha=ew_alpha, k_lim=k_cut, L=L):\n",
    "    \"\"\" Long‐range (reciprocal‐space) Ewald sum (equation A7 and A10 from the paper):\n",
    "    E_recip = (π/V) ∑_{k≠0} [ e^{-k²/(4α)} / k² ] |S(k)|²\n",
    "    with V = L² in 2D, fast convergence.\"\"\"\n",
    "    area, k0 = L*L, 2*pi/L\n",
    "    E=0.0\n",
    "    # 2) Sum over discrete wavevectors q = (m_x, m_y)·(2π/L), the paper has ∑_{q≠0};\n",
    "    # here we loop m_x, m_y ∈ [−k_lim,…,+k_lim]\n",
    "    for mx in range(-k_lim,k_lim+1):\n",
    "        for my in range(-k_lim,k_lim+1):\n",
    "            if mx==0 and my==0: continue # skip the q = 0 term\n",
    "            k = np.array([mx,my])*k0 # q_vec = (m_x, m_y)·(2π/L)\n",
    "            k2 = k@k # q² = |q_vec|² = q_x² + q_y²\n",
    "            E += exp(-k2/(4*alpha))*abs(structure_factor(R,k))**2 / k2 # factor e^{–q²/(4α)} # e^{-q²/(4α)}/q² · |S(q)|²\n",
    "    return (pi/area)*E\n",
    "\n",
    "def self_energy(N, alpha=ew_alpha):\n",
    "    \"\"\" (equation A12 from the paper, Madelung constant) Self‐interaction correction: E_self = - ∑_i (√α / √π) · q_i²\n",
    "    Here q_i are unit charges, so E_self = -N·(√α/√π).\"\"\"\n",
    "    return -sqrt(alpha/pi)*N\n",
    "\n",
    "# ξ_M : configuration-independent Madelung constant\n",
    "def madelung_offset(alpha=ew_alpha,                 # α = 1/(4η²)\n",
    "                    r_lim=r_cut, k_lim=k_cut, L=L):\n",
    "    \"\"\" Compute ξ_M in Eq. (A12) Returns a scalar (dimensionless).  Multiply by e²/4πϵ₀ϵ_r later.\"\"\"\n",
    "    eta   = 0.5 / np.sqrt(alpha)     # because α = 1/(4η²)\n",
    "    area  = L * L\n",
    "    k0    = 2.0 * np.pi / L\n",
    "    # ---- Real-space images   Σ_{L≠0} erfc(|L|/2η)/|L|\n",
    "    rsum = 0.0\n",
    "    maxn = int(np.ceil(r_lim))\n",
    "    for nx in range(-maxn, maxn + 1):\n",
    "        for ny in range(-maxn, maxn + 1):\n",
    "            if nx == 0 and ny == 0:\n",
    "                continue\n",
    "            Rvec = np.array([nx, ny]) * L\n",
    "            R    = np.linalg.norm(Rvec)\n",
    "            if R > r_lim * L:\n",
    "                continue\n",
    "            rsum += erfc(R / (2.0 * eta)) / R\n",
    "    # ---- Reciprocal-space images   (2π/Area) Σ_{G≠0} e^{−η²G²}/G\n",
    "    ksum = 0.0\n",
    "    for mx in range(-k_lim, k_lim + 1):\n",
    "        for my in range(-k_lim, k_lim + 1):\n",
    "            if mx == 0 and my == 0:\n",
    "                continue\n",
    "            Gvec = np.array([mx, my]) * k0\n",
    "            G    = np.linalg.norm(Gvec)\n",
    "            ksum += np.exp(-(eta * G) ** 2) / G\n",
    "    ksum *= 2.0 * np.pi / area\n",
    "    xi0_L = 1.0 / (eta * np.sqrt(np.pi))     # ---- ξ^L_0 term   1 / (η √π)\n",
    "    return rsum + ksum - xi0_L     # ---- ξ_M (Eq. A12)\n",
    "\n",
    "ξ_M = madelung_offset() * e2_4pieps0 / eps_r # compute once and store — units:   meV\n",
    "\n",
    "\n",
    "def coulomb_ewald_2D(R):\n",
    "    \"\"\"\n",
    "    Full 2-D Ewald energy for the set of positions R (shape (N,2)).\n",
    "    Now includes ½ Σ_b ξ_M  so the result matches Eq. (A11) exactly.\n",
    "    E_total = (e²/(4πϵ₀ ε_r))·( E_real + E_recip + E_self )\n",
    "    \"\"\"\n",
    "    N  = len(R)\n",
    "    # position-dependent part (your original implementation)\n",
    "    E_config = (pairwise_real_space(R) +\n",
    "                reciprocal_space(R)   +\n",
    "                self_energy(N)) * e2_4pieps0 / eps_r\n",
    "    # constant Madelung shift\n",
    "    return E_config + 0.5 * N * ξ_M\n",
    "\n",
    "# ---------- public function ----------\n",
    "def energy_static(R):\n",
    "    \"\"\"V_ext + V_ee  (independent of Ψ).\"\"\"\n",
    "    return moire_potential(R) + coulomb_ewald_2D(R)\n",
    "\n",
    "# energy_static(np.random.rand(6,2))\n",
    "\n",
    "# # alias for backward compatibility\n",
    "# energy_moire = energy_static\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    \"\"\" A single feed-forward layer with a tanh activation function.\n",
    "        The input is added to the output of the layer. \"\"\"\n",
    "\n",
    "    def __init__(self, L: int) -> None: # L: layer of width d_L\n",
    "        super().__init__()\n",
    "        self.Wl_1p = nn.Linear(L, L)   # W^(l+1) h^l + b^(l+1)\n",
    "        self.tanh = nn.Tanh()       # (nonlinear) hyperbolic tangent activation function\n",
    "\n",
    "    def forward(self, hl: torch.Tensor) -> torch.Tensor:\n",
    "        return hl + self.tanh(self.Wl_1p(hl))  # input should be of shape (N, L): h^l + tanh( W^(l+1) h^l + b^(l+1) )\n",
    "\n",
    "class SlaterNet(nn.Module):\n",
    "    # def __init__(self, a: float, N: int, L: int = 4, num_layers: int = 3) -> None: # a: lattice constant\n",
    "    def __init__(self, a, N, L = 64, num_layers = 3) -> None:  # L: layer of width d_L; a: lattice constant\n",
    "\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.L = L\n",
    "        self.a = a\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        G_vectors = torch.from_numpy(np.array(b_vectors(a))).float()\n",
    "        self.register_buffer('G1_T', G_vectors[0].unsqueeze(-1))\n",
    "        self.register_buffer('G2_T', G_vectors[1].unsqueeze(-1)) # register_buffer: G vectors as part of the model, but not as a trainable parameter.\n",
    "\n",
    "        # input embedding matrix: projects 4 features to L-dim\n",
    "        self.W_0 = nn.Linear(4, L, bias=False)\n",
    "        self.MLP_layers = nn.ModuleList( # create a list of MLP layers\n",
    "            [FeedForwardLayer(L) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # matrix to hold the projection vectors (complex projectors for orbital) they're trainable parameters\n",
    "        # w_2j and w_2j+1 (one for real one for complex) for j = 0, ... N-1 (6 electrons)\n",
    "        self.complex_proj = nn.Parameter(\n",
    "            torch.complex(real=torch.randn(L, N), imag=torch.randn(L, N))\n",
    "        )\n",
    "        # self.denominator = math.sqrt(math.factorial(N))\n",
    "\n",
    "    def forward(self, R: torch.Tensor) -> torch.Tensor:  # R should be of shape (N, 2)\n",
    "\n",
    "        G1_R = torch.matmul(R, self.G1_T)         # compute the periodic features\n",
    "        G2_R = torch.matmul(R, self.G2_T)\n",
    "        features_R = torch.cat(\n",
    "            (torch.sin(G1_R), torch.sin(G2_R), torch.cos(G1_R), torch.cos(G2_R)), dim=1\n",
    "        ) # shape (N, 4)\n",
    "\n",
    "        # embed in higher_dimensional space to get h^0\n",
    "        h = self.W_0(features_R)\n",
    "\n",
    "        # pass through MLP layers\n",
    "        for layer in self.MLP_layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        # slater matxix\n",
    "        WF_matrix = torch.matmul(h.to(torch.complex64), self.complex_proj)\n",
    "        determinant = torch.linalg.det(WF_matrix)\n",
    "        # result = determinant/self.denominator\n",
    "        return determinant\n",
    "\n",
    "# Checks if you have a GPU (CUDA). If yes, run on GPU for speed; otherwise, run on CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# to handle complex psi (new)\n",
    "def complex_grad(outputs, inputs, grad_outputs=None): # output is ψ, input is R\n",
    "    # Takes the each part of psi and computes its gradient w.r.t. inputs using PyTorch’s autograd.grad\n",
    "    grad_real = torch.autograd.grad(outputs.real, inputs, grad_outputs=grad_outputs, create_graph=True, allow_unused=True)[0]\n",
    "    grad_imag = torch.autograd.grad(outputs.imag, inputs, grad_outputs=grad_outputs, create_graph=True, allow_unused=True)[0]\n",
    "    grad = None\n",
    "    if grad_real is not None and grad_imag is not None:\n",
    "        grad = grad_real + 1j * grad_imag\n",
    "    elif grad_real is not None:\n",
    "        grad = grad_real\n",
    "    elif grad_imag is not None:\n",
    "        grad = 1j * grad_imag\n",
    "    else:\n",
    "        grad = torch.zeros_like(inputs)\n",
    "    return grad # Return the (possibly complex-valued) gradient\n",
    "\n",
    "\n",
    "def compute_laplacian_complex(psi, R):\n",
    "    \"\"\"\n",
    "    Compute ∇²Ψ(R) for a complex Ψ using two successive calls\n",
    "    to complex_grad (which handles complex outputs).\n",
    "    \"\"\"\n",
    "    # first derivatives ∂Ψ/∂R_{i,d}, shape (N_e, 2), complex\n",
    "    grads = complex_grad(psi, R)\n",
    "    lap = 0\n",
    "    N, D = R.shape\n",
    "\n",
    "    # loop electrons i and dims d\n",
    "    for i in range(N):\n",
    "        for d in range(D):\n",
    "            # grab the scalar ∂Ψ/∂R_{i,d}\n",
    "            first_deriv = grads[i, d]\n",
    "\n",
    "            # now take its gradient w.r.t. R\n",
    "            # this is ∂²Ψ / (∂R_{i,d} ∂R_{j,k}) for all j,k\n",
    "            grads2 = complex_grad(first_deriv, R)\n",
    "\n",
    "            # we only want the diagonal piece ∂²Ψ/∂R_{i,d}²\n",
    "            lap += grads2[i, d]\n",
    "\n",
    "    return lap\n",
    "\n",
    "\n",
    "def local_energy(net, R):\n",
    "    R = R.clone().detach().to(device).requires_grad_(True)\n",
    "    psi = net(R)\n",
    "    lap_psi = compute_laplacian_complex(psi, R)\n",
    "    kin_complex = -hbar2_over_2m * (lap_psi / psi)\n",
    "    kin = kin_complex.real\n",
    "    V_np = energy_static(R.detach().cpu().numpy())\n",
    "    V = torch.tensor(V_np, device=device, dtype=kin.dtype)\n",
    "    return kin + V\n",
    "\n",
    "\n",
    "def mcmc_sampler(net, R_init, n_steps=200, step_size=0.1):\n",
    "    \"\"\" Metropolis–Hastings sampler to draw samples ~ |Ψ|².\n",
    "    Inputs:\n",
    "      - net: SlaterNet model\n",
    "      - R_init: torch.Tensor (N_e,2), starting positions\n",
    "      - n_steps: number of MCMC steps\n",
    "      - step_size: Gaussian proposal standard deviation\n",
    "    Returns:\n",
    "      - samples: list of torch.Tensor configurations (one per step)\"\"\"\n",
    "\n",
    "    R = R_init.clone().to(device)\n",
    "    psi_sq = torch.abs(net(R))**2     # Compute initial |Ψ|²\n",
    "    samples = []\n",
    "    for _ in range(n_steps):\n",
    "        for i in range(R.shape[0]):     # propose move for each electron\n",
    "            R_prop = R.clone()\n",
    "            R_prop[i] += step_size * torch.randn_like(R[i])             # Gaussian random displacement\n",
    "            R_prop[i] = R_prop[i] % L       # Enforce PBC in a box of length L\n",
    "            psi_sq_prop = torch.abs(net(R_prop))**2       # evalualte |Ψ|² at the proposed R\n",
    "            if (psi_sq_prop / (psi_sq + 1e-12)) > torch.rand(1, device=device):\n",
    "                R[i] = R_prop[i]\n",
    "                psi_sq = psi_sq_prop\n",
    "        samples.append(R.clone())\n",
    "    return samples\n",
    "\n",
    "\n",
    "def train_vmc(net,\n",
    "              n_iter=200,\n",
    "              n_walkers=8,\n",
    "              n_steps=200,\n",
    "              step_size=0.1):\n",
    "    \"\"\" VMC training with a decaying learning rate: η(t) = η_0 * (1 + t/t0)^(-1) （page 13）\"\"\"\n",
    "\n",
    "    η_0 = 10.0    # initial learning rate (Table II)\n",
    "    # η_0 = 10e-4 \n",
    "    t0  = 1e5     # decay “time constant”\n",
    "    rho = 5.0     # clipping threshold for local energy\n",
    "\n",
    "    optimizer = Adam(net.parameters(), lr=η_0)\n",
    "    # Adam will perform parameter updates θ ← θ – η·∇_θ L\n",
    "\n",
    "    # Each walker is one configuration R of N_e electrons in 2D, drawn uniformly in [0, L)^2\n",
    "    walkers = [L * torch.rand(net.N, 2, device=device) for _ in range(n_walkers)]\n",
    "\n",
    "    # Main VMC loop ──────────────────────────────────────────────────────────────────\n",
    "    for it in range(1, n_iter + 1):\n",
    "        # — Update the learning rate —\n",
    "        lr_t = η_0 * (1 + it / t0) ** -1\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = lr_t\n",
    "\n",
    "        # Prepare lists to collect data over all walkers & steps\n",
    "        batch_E_full = []   # unclipped E_loc, for logging true ⟨E⟩\n",
    "        batch_E_clip = []   # clipped E_loc, for building the loss\n",
    "        batch_logpsi = []   # ln|Ψ(R)|, the “score‐function” term, will be used in loss function\n",
    "        new_walkers  = []   # to hold final config of each walker\n",
    "\n",
    "        for w in walkers:\n",
    "            samples = mcmc_sampler(net, w, n_steps, step_size) # mcmc_sampler we previously wrote\n",
    "            new_walkers.append(samples[-1])\n",
    "            for R in samples:\n",
    "                # 1) compute full local energy\n",
    "                Eloc_full = local_energy(net, R)\n",
    "\n",
    "                # 2) clip for stability in the loss\n",
    "                Eloc_clip = torch.clamp(Eloc_full, -rho, +rho)\n",
    "\n",
    "                batch_E_full.append(Eloc_full)\n",
    "                batch_E_clip.append(Eloc_clip)\n",
    "                batch_logpsi.append(torch.log(torch.abs(net(R)) + 1e-12))\n",
    "\n",
    "        # stack into tensors\n",
    "        E_full      = torch.stack(batch_E_full)\n",
    "        E_clip      = torch.stack(batch_E_clip)\n",
    "        logpsi      = torch.stack(batch_logpsi)\n",
    "\n",
    "        # true mean energy (for reporting)\n",
    "        E_mean_full = E_full.mean()\n",
    "\n",
    "        # clipped mean energy (for loss)\n",
    "        E_mean_clip = E_clip.mean().detach()\n",
    "\n",
    "        # VMC loss uses clipped energies\n",
    "        loss = torch.mean((E_clip - E_mean_clip) * logpsi)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        walkers = new_walkers\n",
    "        print(f'Iter {it:3d}/{n_iter:3d} | <E> = {E_mean_full.item():.6f} meV | lr = {lr_t:.3e}')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfea810",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SlaterNet(a_m, N_e, L=64, num_layers=3).to(device) #-----number layers: 3; Perceptron dim: 64\n",
    "trained_net = train_vmc(net,\n",
    "                        n_iter=20, # paper says 150000 ------- Training iterations\n",
    "                        n_walkers=16,\n",
    "                        n_steps=256,\n",
    "                        step_size=0.05,\n",
    "                        )\n",
    "\n",
    "# ------n_walkers * n_steps = 4096 (MCMC batch size) namely gather 4096 (𝑅,𝐸_loc, lnΨ) where\n",
    "# “n_walkers”：how many independent MCMC chains we run in parallel\n",
    "# “n_steps”: how many successive moves each walker makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbf6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
