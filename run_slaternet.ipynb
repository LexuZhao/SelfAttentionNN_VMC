{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffa1728",
   "metadata": {},
   "source": [
    "## run slaternet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01bd974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from math import erfc, sqrt, pi, exp\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01c31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Simulation constants & helper functions\n",
    "a_m   = 8.031          # nm   moirÃ© lattice constant, supercell length, from paper (the lattice mismatch between WSe2 and WS2)\n",
    "V0    = 15.0         # meV  from paper\n",
    "eps_r = 5.0         # dielectric constant in paper\n",
    "e2_4pieps0 = 14.399645  # meVÂ·nm (|e|Â²/4Ï€Ïµ0)\n",
    "hbar2_over_2m = 108.857 # meVÂ·nmÂ² in paper\n",
    "phi = np.pi/4   # phase of the moirÃ© potential from paper\n",
    "n_sup = 3            # 3Ã—3 super-cell\n",
    "N_e   = 6               # # electrons (= # occupied orbitals)\n",
    "cut   = 2  # |G|â‰¤cutÂ·|g| # cutoff for the plane-wave basis (G vectors) in the Hamiltonian\n",
    "\n",
    "# --- Ewald parameters (renamed) ---\n",
    "ew_alpha = 0.35      # nmâ»Â²  (splitting)\n",
    "r_cut    = 2.5       # real-space cutoff in L units\n",
    "k_cut    = 5         # k-space cutoff in 2Ï€/L units\n",
    "L     = n_sup * a_m  # nm   PBC box length used in Ewald\n",
    "\n",
    "def a_vectors(a_m):\n",
    "    \"\"\"Generates the 3 shortest moirÃ© reciprocal vectors G_1,2,3, 60Â° apart, six-fold symmetry\"\"\"\n",
    "    a1 = a_m * np.array([1.0,                0.0           ])\n",
    "    a2 = a_m * np.array([0.5,  np.sqrt(3.0) / 2.0          ])\n",
    "    a3 = -(a1 + a2)        # optional third vector (120Â° w.r.t. a1)\n",
    "    return [a1, a2, a3]    # same interface style as your b_vectors()\n",
    "\n",
    "\n",
    "def b_vectors(a_m):\n",
    "    \"\"\"from paper: g_j = (4*pi / sqrt(3) / a_m) * [cos(2*pi*j/3), sin(2*pi*j/3)], for j=1,2,3\"\"\"\n",
    "    g_list = []\n",
    "    prefac = 4 * np.pi / (np.sqrt(3) * a_m)\n",
    "    for j in range(1, 4):  # j = 1, 2, 3\n",
    "        angle = 2 * np.pi * j / 3\n",
    "        g = prefac * np.array([np.cos(angle), np.sin(angle)])\n",
    "        g_list.append(g)\n",
    "    # print(\"g list\", g_list)\n",
    "    return g_list  # returns [g1, g2, g3]\n",
    "\n",
    "# real-space vectors of an nÃ—n super-cell   (n = 3 here since we have 3x3 supercell)\n",
    "def supercell_vectors(n, a_m):\n",
    "    a1, a2 = a_vectors(a_m)\n",
    "    return n*a1, n*a2\n",
    "\n",
    "\n",
    "def moire_potential(r, a_m = a_m, V0 = V0, phi = phi):\n",
    "    \"\"\" V(r) = -2*V0*sum_{j=1}^{3} cos(g_j Â· r + phi)where g_j are 3 reciprocal lattice vectors (from paper).\"\"\"\n",
    "    G = np.array(b_vectors(a_m))  # Get the three reciprocal vectors, shape (3,2)\n",
    "    phase = np.dot(r, G.T) + phi  # r @ G.T + phi\n",
    "    one_electron_moire = -2 * V0 * np.sum(np.cos(phase), axis=-1)\n",
    "    return np.sum(one_electron_moire)\n",
    "\n",
    "# ---------- Ewald helpers (using ew_alpha) ----------\n",
    "def pairwise_real_space(R, alpha=ew_alpha, r_lim=r_cut, L=L):\n",
    "    \"\"\" Shortâ€range (realâ€space) Ewald sum (equation A6 from the paper):\n",
    "    E_real = Â½ âˆ‘_{iâ‰ j} âˆ‘_L erfc(âˆšÎ±Â·r_{ij}^L) / r_{ij}^L.,\n",
    "    where Î± = 1/(4Î·Â²), and r_{ij}^L = |r_i - r_j + L|\"\"\"\n",
    "    N, E = len(R), 0.0 #N:number particles\n",
    "    maxn = int(np.ceil(r_lim)) # summing over neighbor cells from -max_n to +max_n\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N): # loop over Â½ âˆ‘_{iâ‰ j}\n",
    "            for nx in range(-maxn,maxn+1): #loop over n_x n_y\n",
    "                for ny in range(-maxn,maxn+1):\n",
    "                    dr = R[i]-R[j]+np.array([nx,ny])*L # dr = r_i - r_j + nÂ·L\n",
    "                    r  = np.linalg.norm(dr) # r = |dr|\n",
    "                    if r<1e-9 or r>r_lim*L: continue # Skip selfâ€interaction (râ‰ˆ0) or beyond cutoff r_limÂ·L\n",
    "                    E += erfc(sqrt(alpha)*r)/r # Î± = 1/(4Î·Â²) we choose\n",
    "    return E\n",
    "\n",
    "def structure_factor(R,k):    # Î£ e^{ikÂ·r}\n",
    "    \"\"\"Structure factor S(k) = Î£ e^{ikÂ·r} (sum over all particles)\"\"\"\n",
    "    phase = R @ k\n",
    "    return np.sum(np.cos(phase))+1j*np.sum(np.sin(phase))\n",
    "\n",
    "def reciprocal_space(R, alpha=ew_alpha, k_lim=k_cut, L=L):\n",
    "    \"\"\" Longâ€range (reciprocalâ€space) Ewald sum (equation A7 and A10 from the paper):\n",
    "    E_recip = (Ï€/V) âˆ‘_{kâ‰ 0} [ e^{-kÂ²/(4Î±)} / kÂ² ] |S(k)|Â²\n",
    "    with V = LÂ² in 2D, fast convergence.\"\"\"\n",
    "    area, k0 = L*L, 2*pi/L\n",
    "    E=0.0\n",
    "    # 2) Sum over discrete wavevectors q = (m_x, m_y)Â·(2Ï€/L), the paper has âˆ‘_{qâ‰ 0};\n",
    "    # here we loop m_x, m_y âˆˆ [âˆ’k_lim,â€¦,+k_lim]\n",
    "    for mx in range(-k_lim,k_lim+1):\n",
    "        for my in range(-k_lim,k_lim+1):\n",
    "            if mx==0 and my==0: continue # skip the q = 0 term\n",
    "            k = np.array([mx,my])*k0 # q_vec = (m_x, m_y)Â·(2Ï€/L)\n",
    "            k2 = k@k # qÂ² = |q_vec|Â² = q_xÂ² + q_yÂ²\n",
    "            E += exp(-k2/(4*alpha))*abs(structure_factor(R,k))**2 / k2 # factor e^{â€“qÂ²/(4Î±)} # e^{-qÂ²/(4Î±)}/qÂ² Â· |S(q)|Â²\n",
    "    return (pi/area)*E\n",
    "\n",
    "def self_energy(N, alpha=ew_alpha):\n",
    "    \"\"\" (equation A12 from the paper, Madelung constant) Selfâ€interaction correction: E_self = - âˆ‘_i (âˆšÎ± / âˆšÏ€) Â· q_iÂ²\n",
    "    Here q_i are unit charges, so E_self = -NÂ·(âˆšÎ±/âˆšÏ€).\"\"\"\n",
    "    return -sqrt(alpha/pi)*N\n",
    "\n",
    "# Î¾_M : configuration-independent Madelung constant\n",
    "def madelung_offset(alpha=ew_alpha,                 # Î± = 1/(4Î·Â²)\n",
    "                    r_lim=r_cut, k_lim=k_cut, L=L):\n",
    "    \"\"\" Compute Î¾_M in Eq. (A12) Returns a scalar (dimensionless).  Multiply by eÂ²/4Ï€Ïµâ‚€Ïµ_r later.\"\"\"\n",
    "    eta   = 0.5 / np.sqrt(alpha)     # because Î± = 1/(4Î·Â²)\n",
    "    area  = L * L\n",
    "    k0    = 2.0 * np.pi / L\n",
    "    # ---- Real-space images   Î£_{Lâ‰ 0} erfc(|L|/2Î·)/|L|\n",
    "    rsum = 0.0\n",
    "    maxn = int(np.ceil(r_lim))\n",
    "    for nx in range(-maxn, maxn + 1):\n",
    "        for ny in range(-maxn, maxn + 1):\n",
    "            if nx == 0 and ny == 0:\n",
    "                continue\n",
    "            Rvec = np.array([nx, ny]) * L\n",
    "            R    = np.linalg.norm(Rvec)\n",
    "            if R > r_lim * L:\n",
    "                continue\n",
    "            rsum += erfc(R / (2.0 * eta)) / R\n",
    "    # ---- Reciprocal-space images   (2Ï€/Area) Î£_{Gâ‰ 0} e^{âˆ’Î·Â²GÂ²}/G\n",
    "    ksum = 0.0\n",
    "    for mx in range(-k_lim, k_lim + 1):\n",
    "        for my in range(-k_lim, k_lim + 1):\n",
    "            if mx == 0 and my == 0:\n",
    "                continue\n",
    "            Gvec = np.array([mx, my]) * k0\n",
    "            G    = np.linalg.norm(Gvec)\n",
    "            ksum += np.exp(-(eta * G) ** 2) / G\n",
    "    ksum *= 2.0 * np.pi / area\n",
    "    xi0_L = 1.0 / (eta * np.sqrt(np.pi))     # ---- Î¾^L_0 term   1 / (Î· âˆšÏ€)\n",
    "    return rsum + ksum - xi0_L     # ---- Î¾_M (Eq. A12)\n",
    "\n",
    "Î¾_M = madelung_offset() * e2_4pieps0 / eps_r # compute once and store â€” units:   meV\n",
    "\n",
    "\n",
    "def coulomb_ewald_2D(R):\n",
    "    \"\"\"\n",
    "    Full 2-D Ewald energy for the set of positions R (shape (N,2)).\n",
    "    Now includes Â½ Î£_b Î¾_M  so the result matches Eq. (A11) exactly.\n",
    "    E_total = (eÂ²/(4Ï€Ïµâ‚€ Îµ_r))Â·( E_real + E_recip + E_self )\n",
    "    \"\"\"\n",
    "    N  = len(R)\n",
    "    # position-dependent part (your original implementation)\n",
    "    E_config = (pairwise_real_space(R) +\n",
    "                reciprocal_space(R)   +\n",
    "                self_energy(N)) * e2_4pieps0 / eps_r\n",
    "    # constant Madelung shift\n",
    "    return E_config + 0.5 * N * Î¾_M\n",
    "\n",
    "# ---------- public function ----------\n",
    "def energy_static(R):\n",
    "    \"\"\"V_ext + V_ee  (independent of Î¨).\"\"\"\n",
    "    return moire_potential(R) + coulomb_ewald_2D(R)\n",
    "\n",
    "# energy_static(np.random.rand(6,2))\n",
    "\n",
    "# # alias for backward compatibility\n",
    "# energy_moire = energy_static\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    \"\"\" A single feed-forward layer with a tanh activation function.\n",
    "        The input is added to the output of the layer. \"\"\"\n",
    "\n",
    "    def __init__(self, L: int) -> None: # L: layer of width d_L\n",
    "        super().__init__()\n",
    "        self.Wl_1p = nn.Linear(L, L)   # W^(l+1) h^l + b^(l+1)\n",
    "        self.tanh = nn.Tanh()       # (nonlinear) hyperbolic tangent activation function\n",
    "\n",
    "    def forward(self, hl: torch.Tensor) -> torch.Tensor:\n",
    "        return hl + self.tanh(self.Wl_1p(hl))  # input should be of shape (N, L): h^l + tanh( W^(l+1) h^l + b^(l+1) )\n",
    "\n",
    "class SlaterNet(nn.Module):\n",
    "    # def __init__(self, a: float, N: int, L: int = 4, num_layers: int = 3) -> None: # a: lattice constant\n",
    "    def __init__(self, a, N, L = 64, num_layers = 3) -> None:  # L: layer of width d_L; a: lattice constant\n",
    "\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.L = L\n",
    "        self.a = a\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        G_vectors = torch.from_numpy(np.array(b_vectors(a))).float()\n",
    "        self.register_buffer('G1_T', G_vectors[0].unsqueeze(-1))\n",
    "        self.register_buffer('G2_T', G_vectors[1].unsqueeze(-1)) # register_buffer: G vectors as part of the model, but not as a trainable parameter.\n",
    "\n",
    "        # input embedding matrix: projects 4 features to L-dim\n",
    "        self.W_0 = nn.Linear(4, L, bias=False)\n",
    "        self.MLP_layers = nn.ModuleList( # create a list of MLP layers\n",
    "            [FeedForwardLayer(L) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # matrix to hold the projection vectors (complex projectors for orbital) they're trainable parameters\n",
    "        # w_2j and w_2j+1 (one for real one for complex) for j = 0, ... N-1 (6 electrons)\n",
    "        self.complex_proj = nn.Parameter(\n",
    "            torch.complex(real=torch.randn(L, N), imag=torch.randn(L, N))\n",
    "        )\n",
    "        # self.denominator = math.sqrt(math.factorial(N))\n",
    "\n",
    "    def forward(self, R: torch.Tensor) -> torch.Tensor:  # R should be of shape (N, 2)\n",
    "\n",
    "        G1_R = torch.matmul(R, self.G1_T)         # compute the periodic features\n",
    "        G2_R = torch.matmul(R, self.G2_T)\n",
    "        features_R = torch.cat(\n",
    "            (torch.sin(G1_R), torch.sin(G2_R), torch.cos(G1_R), torch.cos(G2_R)), dim=1\n",
    "        ) # shape (N, 4)\n",
    "\n",
    "        # embed in higher_dimensional space to get h^0\n",
    "        h = self.W_0(features_R)\n",
    "\n",
    "        # pass through MLP layers\n",
    "        for layer in self.MLP_layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        # slater matxix\n",
    "        WF_matrix = torch.matmul(h.to(torch.complex64), self.complex_proj)\n",
    "        determinant = torch.linalg.det(WF_matrix)\n",
    "        # result = determinant/self.denominator\n",
    "        return determinant\n",
    "\n",
    "# Checks if you have a GPU (CUDA). If yes, run on GPU for speed; otherwise, run on CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# to handle complex psi (new)\n",
    "def complex_grad(outputs, inputs, grad_outputs=None): # output is Ïˆ, input is R\n",
    "    # Takes the each part of psi and computes its gradient w.r.t. inputs using PyTorchâ€™s autograd.grad\n",
    "    grad_real = torch.autograd.grad(outputs.real, inputs, grad_outputs=grad_outputs, create_graph=True, allow_unused=True)[0]\n",
    "    grad_imag = torch.autograd.grad(outputs.imag, inputs, grad_outputs=grad_outputs, create_graph=True, allow_unused=True)[0]\n",
    "    grad = None\n",
    "    if grad_real is not None and grad_imag is not None:\n",
    "        grad = grad_real + 1j * grad_imag\n",
    "    elif grad_real is not None:\n",
    "        grad = grad_real\n",
    "    elif grad_imag is not None:\n",
    "        grad = 1j * grad_imag\n",
    "    else:\n",
    "        grad = torch.zeros_like(inputs)\n",
    "    return grad # Return the (possibly complex-valued) gradient\n",
    "\n",
    "\n",
    "def compute_laplacian_complex(psi, R):\n",
    "    \"\"\"\n",
    "    Compute âˆ‡Â²Î¨(R) for a complex Î¨ using two successive calls\n",
    "    to complex_grad (which handles complex outputs).\n",
    "    \"\"\"\n",
    "    # first derivatives âˆ‚Î¨/âˆ‚R_{i,d}, shape (N_e, 2), complex\n",
    "    grads = complex_grad(psi, R)\n",
    "    lap = 0\n",
    "    N, D = R.shape\n",
    "\n",
    "    # loop electrons i and dims d\n",
    "    for i in range(N):\n",
    "        for d in range(D):\n",
    "            # grab the scalar âˆ‚Î¨/âˆ‚R_{i,d}\n",
    "            first_deriv = grads[i, d]\n",
    "\n",
    "            # now take its gradient w.r.t. R\n",
    "            # this is âˆ‚Â²Î¨ / (âˆ‚R_{i,d} âˆ‚R_{j,k}) for all j,k\n",
    "            grads2 = complex_grad(first_deriv, R)\n",
    "\n",
    "            # we only want the diagonal piece âˆ‚Â²Î¨/âˆ‚R_{i,d}Â²\n",
    "            lap += grads2[i, d]\n",
    "\n",
    "    return lap\n",
    "\n",
    "\n",
    "def local_energy(net, R):\n",
    "    R = R.clone().detach().to(device).requires_grad_(True)\n",
    "    psi = net(R)\n",
    "    lap_psi = compute_laplacian_complex(psi, R)\n",
    "    kin_complex = -hbar2_over_2m * (lap_psi / psi)\n",
    "    kin = kin_complex.real\n",
    "    V_np = energy_static(R.detach().cpu().numpy())\n",
    "    V = torch.tensor(V_np, device=device, dtype=kin.dtype)\n",
    "    return kin + V\n",
    "\n",
    "\n",
    "def mcmc_sampler(net, R_init, n_steps=200, step_size=0.1):\n",
    "    \"\"\" Metropolisâ€“Hastings sampler to draw samples ~ |Î¨|Â².\n",
    "    Inputs:\n",
    "      - net: SlaterNet model\n",
    "      - R_init: torch.Tensor (N_e,2), starting positions\n",
    "      - n_steps: number of MCMC steps\n",
    "      - step_size: Gaussian proposal standard deviation\n",
    "    Returns:\n",
    "      - samples: list of torch.Tensor configurations (one per step)\"\"\"\n",
    "\n",
    "    R = R_init.clone().to(device)\n",
    "    psi_sq = torch.abs(net(R))**2     # Compute initial |Î¨|Â²\n",
    "    samples = []\n",
    "    for _ in range(n_steps):\n",
    "        for i in range(R.shape[0]):     # propose move for each electron\n",
    "            R_prop = R.clone()\n",
    "            R_prop[i] += step_size * torch.randn_like(R[i])             # Gaussian random displacement\n",
    "            R_prop[i] = R_prop[i] % L       # Enforce PBC in a box of length L\n",
    "            psi_sq_prop = torch.abs(net(R_prop))**2       # evalualte |Î¨|Â² at the proposed R\n",
    "            if (psi_sq_prop / (psi_sq + 1e-12)) > torch.rand(1, device=device):\n",
    "                R[i] = R_prop[i]\n",
    "                psi_sq = psi_sq_prop\n",
    "        samples.append(R.clone())\n",
    "    return samples\n",
    "\n",
    "\n",
    "def train_vmc(net,\n",
    "              n_iter=200,\n",
    "              n_walkers=8,\n",
    "              n_steps=200,\n",
    "              step_size=0.1):\n",
    "    \"\"\" VMC training with a decaying learning rate: Î·(t) = Î·_0 * (1 + t/t0)^(-1) ï¼ˆpage 13ï¼‰\"\"\"\n",
    "\n",
    "    Î·_0 = 10.0    # initial learning rate (Table II)\n",
    "    # Î·_0 = 10e-4 \n",
    "    t0  = 1e5     # decay â€œtime constantâ€\n",
    "    rho = 5.0     # clipping threshold for local energy\n",
    "\n",
    "    optimizer = Adam(net.parameters(), lr=Î·_0)\n",
    "    # Adam will perform parameter updates Î¸ â† Î¸ â€“ Î·Â·âˆ‡_Î¸ L\n",
    "\n",
    "    # Each walker is one configuration R of N_e electrons in 2D, drawn uniformly in [0, L)^2\n",
    "    walkers = [L * torch.rand(net.N, 2, device=device) for _ in range(n_walkers)]\n",
    "\n",
    "    # Main VMC loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for it in range(1, n_iter + 1):\n",
    "        # â€” Update the learning rate â€”\n",
    "        lr_t = Î·_0 * (1 + it / t0) ** -1\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = lr_t\n",
    "\n",
    "        # Prepare lists to collect data over all walkers & steps\n",
    "        batch_E_full = []   # unclipped E_loc, for logging true âŸ¨EâŸ©\n",
    "        batch_E_clip = []   # clipped E_loc, for building the loss\n",
    "        batch_logpsi = []   # ln|Î¨(R)|, the â€œscoreâ€functionâ€ term, will be used in loss function\n",
    "        new_walkers  = []   # to hold final config of each walker\n",
    "\n",
    "        for w in walkers:\n",
    "            samples = mcmc_sampler(net, w, n_steps, step_size) # mcmc_sampler we previously wrote\n",
    "            new_walkers.append(samples[-1])\n",
    "            for R in samples:\n",
    "                # 1) compute full local energy\n",
    "                Eloc_full = local_energy(net, R)\n",
    "\n",
    "                # 2) clip for stability in the loss\n",
    "                Eloc_clip = torch.clamp(Eloc_full, -rho, +rho)\n",
    "\n",
    "                batch_E_full.append(Eloc_full)\n",
    "                batch_E_clip.append(Eloc_clip)\n",
    "                batch_logpsi.append(torch.log(torch.abs(net(R)) + 1e-12))\n",
    "\n",
    "        # stack into tensors\n",
    "        E_full      = torch.stack(batch_E_full)\n",
    "        E_clip      = torch.stack(batch_E_clip)\n",
    "        logpsi      = torch.stack(batch_logpsi)\n",
    "\n",
    "        # true mean energy (for reporting)\n",
    "        E_mean_full = E_full.mean()\n",
    "\n",
    "        # clipped mean energy (for loss)\n",
    "        E_mean_clip = E_clip.mean().detach()\n",
    "\n",
    "        # VMC loss uses clipped energies\n",
    "        loss = torch.mean((E_clip - E_mean_clip) * logpsi)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        walkers = new_walkers\n",
    "        print(f'Iter {it:3d}/{n_iter:3d} | <E> = {E_mean_full.item():.6f} meV | lr = {lr_t:.3e}')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfea810",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SlaterNet(a_m, N_e, L=64, num_layers=3).to(device) #-----number layers: 3; Perceptron dim: 64\n",
    "trained_net = train_vmc(net,\n",
    "                        n_iter=20, # paper says 150000 ------- Training iterations\n",
    "                        n_walkers=16,\n",
    "                        n_steps=256,\n",
    "                        step_size=0.05,\n",
    "                        )\n",
    "\n",
    "# ------n_walkers * n_steps = 4096 (MCMC batch size) namely gather 4096 (ğ‘…,ğ¸_loc, lnÎ¨) where\n",
    "# â€œn_walkersâ€ï¼šhow many independent MCMC chains we run in parallel\n",
    "# â€œn_stepsâ€: how many successive moves each walker makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbf6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
